{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from scipy.interpolate import interp1d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "############# Note that the input of this model is the downsampled data. I opened the data in matlab and downsampled them using \n",
    "#the downsample(train_data_long_0var,1200) and then export them as csv files and the result is the input of this function\n",
    "\n",
    "def pm2d5_pred_model(train_data, test_data, problem_type):\n",
    "    \n",
    "    if problem_type==2:\n",
    "        df=train_data.dropna()\n",
    "        test=test_data\n",
    "        dataset = df.values\n",
    "        split_fraction = 0.8\n",
    "        train_split = int(split_fraction * int(df.shape[0]))\n",
    "        data_mean = df[:train_split].mean(axis=0)\n",
    "        data_std = df[:train_split].std(axis=0)\n",
    "        def normalize(data, train_split):\n",
    "            data_mean = data[:train_split].mean(axis=0)\n",
    "            data_std = data[:train_split].std(axis=0)\n",
    "            return (data - data_mean) / data_std\n",
    "        features_key = [\n",
    "            \"hmd\",\n",
    "            \"spd\",\n",
    "            \"tmp\",\n",
    "            \"pm2d5\",\n",
    "        ]\n",
    "\n",
    "\n",
    "        date_time_key = \"time\"\n",
    "        selected_features = [features_key[i] for i in [0, 1, 2, 3]]\n",
    "        selected_features1 = [features_key[i] for i in [0, 1, 2]]\n",
    "        features = df[selected_features]\n",
    "        features.index = df[date_time_key]\n",
    "        features = normalize(features.values, train_split)\n",
    "        features = pd.DataFrame(features)\n",
    "        norm = test[selected_features1]\n",
    "        norm.index = test[date_time_key]\n",
    "        norm = normalize(norm.values, test.shape[0])\n",
    "        norm = pd.DataFrame(norm)\n",
    "        train_data = features.loc[0 : train_split - 1]\n",
    "        test_data = features.loc[train_split:]\n",
    "        train_split1 = int(0.5* int(test_data.shape[0]))\n",
    "        test_data1 = features.loc[train_split : train_split+train_split1 - 1]\n",
    "        val_data = features.loc[train_split1+train_split:]\n",
    "        tes=norm.loc[:]\n",
    "        start = 1\n",
    "        batch_size = 512 \n",
    "        epochs = 10\n",
    "        step = 1\n",
    "        end = start + train_split\n",
    "        x_train = train_data[[i for i in range(3)]].values\n",
    "        y_train = features.iloc[start:end][[3]]\n",
    "        sequence_length = 2\n",
    "        x_end = len(val_data)+len(test_data1) - start\n",
    "        label_start = train_split+train_split1 + start-1\n",
    "        x_val = val_data.iloc[:x_end][[i for i in range(3)]].values\n",
    "        y_val = features.iloc[label_start:][[3]]\n",
    "        dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            x_val,\n",
    "            y_val,\n",
    "            sequence_length=sequence_length,\n",
    "            sampling_rate=step,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        for batch in dataset_val.take(100):\n",
    "            inputs, targets = batch\n",
    "        dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            sequence_length=sequence_length,\n",
    "            sampling_rate=step,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        end = start + train_split1\n",
    "        x_test = tes[[i for i in range(3)]].values\n",
    "        y_test = features.iloc[start:len(x_test)+1][[3]]\n",
    "        for batch in dataset_val.take(100):\n",
    "            inputs, targets = batch\n",
    "        dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            x_test,\n",
    "            y_test,\n",
    "            sequence_length=sequence_length,\n",
    "            sampling_rate=step,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(80, activation='relu', input_shape=(162, 3)))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='SGD', loss='mse')\n",
    "        model.fit(dataset_train,\n",
    "                  epochs=150,\n",
    "                  validation_data=dataset_val,verbose=1, batch_size=128)\n",
    "        predicted = model.predict(dataset_test)\n",
    "        pred= pd.DataFrame({'Column1': predicted[:, 0]})\n",
    "        new_index = pred.index[-1] + 1\n",
    "        pred = pred.append(pd.DataFrame(index=[new_index], data=pred.tail(1).values, columns=pred.columns))\n",
    "        return (pred*data_std['pm2d5'])+data_mean['pm2d5']\n",
    "    else:\n",
    "        print(\"this model is for problem type 2-longterm-prediction\")\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_long_10var=pd.read_csv('train_data_long_10var.csv')\n",
    "test_data_long_10var=pd.read_csv('test_data_long_10var.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3/3 [==============================] - 2s 234ms/step - loss: 0.9998 - val_loss: 0.6259\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.9977 - val_loss: 0.6231\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.9956 - val_loss: 0.6206\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.9935 - val_loss: 0.6185\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.9916 - val_loss: 0.6169\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.9901 - val_loss: 0.6157\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.9888 - val_loss: 0.6148\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.9876 - val_loss: 0.6139\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9865 - val_loss: 0.6130\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.9854 - val_loss: 0.6121\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.9843 - val_loss: 0.6112\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.9833 - val_loss: 0.6104\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.9824 - val_loss: 0.6096\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.9814 - val_loss: 0.6089\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.9805 - val_loss: 0.6083\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.9796 - val_loss: 0.6076\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.9788 - val_loss: 0.6071\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.9780 - val_loss: 0.6067\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9772 - val_loss: 0.6063\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.9764 - val_loss: 0.6061\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.9756 - val_loss: 0.6060\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.9749 - val_loss: 0.6059\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.9742 - val_loss: 0.6059\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.9735 - val_loss: 0.6058\n",
      "Epoch 25/150\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.9728 - val_loss: 0.6057\n",
      "Epoch 26/150\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.9722 - val_loss: 0.6057\n",
      "Epoch 27/150\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.9715 - val_loss: 0.6058\n",
      "Epoch 28/150\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.9710 - val_loss: 0.6059\n",
      "Epoch 29/150\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.9704 - val_loss: 0.6060\n",
      "Epoch 30/150\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.9699 - val_loss: 0.6061\n",
      "Epoch 31/150\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.9694 - val_loss: 0.6062\n",
      "Epoch 32/150\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.9689 - val_loss: 0.6063\n",
      "Epoch 33/150\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.9684 - val_loss: 0.6064\n",
      "Epoch 34/150\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.9680 - val_loss: 0.6065\n",
      "Epoch 35/150\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.9675 - val_loss: 0.6067\n",
      "Epoch 36/150\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.9671 - val_loss: 0.6069\n",
      "Epoch 37/150\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9667 - val_loss: 0.6071\n",
      "Epoch 38/150\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.9663 - val_loss: 0.6073\n",
      "Epoch 39/150\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.9659 - val_loss: 0.6075\n",
      "Epoch 40/150\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.9655 - val_loss: 0.6076\n",
      "Epoch 41/150\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9652 - val_loss: 0.6078\n",
      "Epoch 42/150\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.9648 - val_loss: 0.6080\n",
      "Epoch 43/150\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.9645 - val_loss: 0.6082\n",
      "Epoch 44/150\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9642 - val_loss: 0.6084\n",
      "Epoch 45/150\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.9639 - val_loss: 0.6086\n",
      "Epoch 46/150\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9636 - val_loss: 0.6088\n",
      "Epoch 47/150\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.9633 - val_loss: 0.6090\n",
      "Epoch 48/150\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9630 - val_loss: 0.6093\n",
      "Epoch 49/150\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.9627 - val_loss: 0.6095\n",
      "Epoch 50/150\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.9624 - val_loss: 0.6097\n",
      "Epoch 51/150\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.9621 - val_loss: 0.6099\n",
      "Epoch 52/150\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.9618 - val_loss: 0.6101\n",
      "Epoch 53/150\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.9615 - val_loss: 0.6103\n",
      "Epoch 54/150\n",
      "3/3 [==============================] - 0s 156ms/step - loss: 0.9613 - val_loss: 0.6104\n",
      "Epoch 55/150\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.9610 - val_loss: 0.6106\n",
      "Epoch 56/150\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.9607 - val_loss: 0.6107\n",
      "Epoch 57/150\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.9605 - val_loss: 0.6109\n",
      "Epoch 58/150\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.9602 - val_loss: 0.6110\n",
      "Epoch 59/150\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.9600 - val_loss: 0.6112\n",
      "Epoch 60/150\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.9597 - val_loss: 0.6114\n",
      "Epoch 61/150\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 0.9595 - val_loss: 0.6116\n",
      "Epoch 62/150\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.9593 - val_loss: 0.6117\n",
      "Epoch 63/150\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 0.9590 - val_loss: 0.6119\n",
      "Epoch 64/150\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.9588 - val_loss: 0.6120\n",
      "Epoch 65/150\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.9586 - val_loss: 0.6121\n",
      "Epoch 66/150\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.9584 - val_loss: 0.6123\n",
      "Epoch 67/150\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.9581 - val_loss: 0.6124\n",
      "Epoch 68/150\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.9579 - val_loss: 0.6126\n",
      "Epoch 69/150\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.9577 - val_loss: 0.6128\n",
      "Epoch 70/150\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.9575 - val_loss: 0.6130\n",
      "Epoch 71/150\n",
      "3/3 [==============================] - 1s 340ms/step - loss: 0.9573 - val_loss: 0.6132\n",
      "Epoch 72/150\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.9571 - val_loss: 0.6133\n",
      "Epoch 73/150\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.9569 - val_loss: 0.6135\n",
      "Epoch 74/150\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.9567 - val_loss: 0.6136\n",
      "Epoch 75/150\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.9565 - val_loss: 0.6137\n",
      "Epoch 76/150\n",
      "3/3 [==============================] - 1s 359ms/step - loss: 0.9563 - val_loss: 0.6139\n",
      "Epoch 77/150\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 0.9561 - val_loss: 0.6140\n",
      "Epoch 78/150\n",
      "3/3 [==============================] - 0s 160ms/step - loss: 0.9559 - val_loss: 0.6142\n",
      "Epoch 79/150\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 0.9557 - val_loss: 0.6143\n",
      "Epoch 80/150\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.9555 - val_loss: 0.6145\n",
      "Epoch 81/150\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.9553 - val_loss: 0.6146\n",
      "Epoch 82/150\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.9551 - val_loss: 0.6147\n",
      "Epoch 83/150\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.9548 - val_loss: 0.6148\n",
      "Epoch 84/150\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.9546 - val_loss: 0.6149\n",
      "Epoch 85/150\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.9545 - val_loss: 0.6150\n",
      "Epoch 86/150\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 0.9542 - val_loss: 0.6151\n",
      "Epoch 87/150\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.9540 - val_loss: 0.6153\n",
      "Epoch 88/150\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.9539 - val_loss: 0.6154\n",
      "Epoch 89/150\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.9537 - val_loss: 0.6155\n",
      "Epoch 90/150\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.9535 - val_loss: 0.6156\n",
      "Epoch 91/150\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.9533 - val_loss: 0.6158\n",
      "Epoch 92/150\n",
      "3/3 [==============================] - 0s 204ms/step - loss: 0.9531 - val_loss: 0.6159\n",
      "Epoch 93/150\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.9529 - val_loss: 0.6161\n",
      "Epoch 94/150\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.9527 - val_loss: 0.6161\n",
      "Epoch 95/150\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.9525 - val_loss: 0.6162\n",
      "Epoch 96/150\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.9523 - val_loss: 0.6163\n",
      "Epoch 97/150\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.9522 - val_loss: 0.6163\n",
      "Epoch 98/150\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.9520 - val_loss: 0.6164\n",
      "Epoch 99/150\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.9518 - val_loss: 0.6164\n",
      "Epoch 100/150\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.9517 - val_loss: 0.6165\n",
      "Epoch 101/150\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.9515 - val_loss: 0.6166\n",
      "Epoch 102/150\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.9514 - val_loss: 0.6166\n",
      "Epoch 103/150\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.9512 - val_loss: 0.6167\n",
      "Epoch 104/150\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.9510 - val_loss: 0.6167\n",
      "Epoch 105/150\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.9509 - val_loss: 0.6168\n",
      "Epoch 106/150\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.9507 - val_loss: 0.6168\n",
      "Epoch 107/150\n",
      "3/3 [==============================] - 0s 161ms/step - loss: 0.9506 - val_loss: 0.6168\n",
      "Epoch 108/150\n",
      "3/3 [==============================] - 0s 186ms/step - loss: 0.9504 - val_loss: 0.6168\n",
      "Epoch 109/150\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.9503 - val_loss: 0.6168\n",
      "Epoch 110/150\n",
      "3/3 [==============================] - 0s 188ms/step - loss: 0.9501 - val_loss: 0.6168\n",
      "Epoch 111/150\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.9500 - val_loss: 0.6168\n",
      "Epoch 112/150\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.9498 - val_loss: 0.6168\n",
      "Epoch 113/150\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 0.9497 - val_loss: 0.6167\n",
      "Epoch 114/150\n",
      "3/3 [==============================] - 0s 190ms/step - loss: 0.9495 - val_loss: 0.6167\n",
      "Epoch 115/150\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.9494 - val_loss: 0.6167\n",
      "Epoch 116/150\n",
      "3/3 [==============================] - 1s 213ms/step - loss: 0.9492 - val_loss: 0.6167\n",
      "Epoch 117/150\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.9491 - val_loss: 0.6167\n",
      "Epoch 118/150\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.9489 - val_loss: 0.6166\n",
      "Epoch 119/150\n",
      "3/3 [==============================] - 0s 211ms/step - loss: 0.9488 - val_loss: 0.6166\n",
      "Epoch 120/150\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.9486 - val_loss: 0.6166\n",
      "Epoch 121/150\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.9485 - val_loss: 0.6165\n",
      "Epoch 122/150\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.9483 - val_loss: 0.6165\n",
      "Epoch 123/150\n",
      "3/3 [==============================] - 0s 168ms/step - loss: 0.9482 - val_loss: 0.6165\n",
      "Epoch 124/150\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.9480 - val_loss: 0.6164\n",
      "Epoch 125/150\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.9479 - val_loss: 0.6164\n",
      "Epoch 126/150\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.9477 - val_loss: 0.6164\n",
      "Epoch 127/150\n",
      "3/3 [==============================] - 0s 219ms/step - loss: 0.9476 - val_loss: 0.6163\n",
      "Epoch 128/150\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.9475 - val_loss: 0.6162\n",
      "Epoch 129/150\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.9473 - val_loss: 0.6162\n",
      "Epoch 130/150\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.9472 - val_loss: 0.6161\n",
      "Epoch 131/150\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.9471 - val_loss: 0.6161\n",
      "Epoch 132/150\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.9469 - val_loss: 0.6160\n",
      "Epoch 133/150\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.9468 - val_loss: 0.6159\n",
      "Epoch 134/150\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.9466 - val_loss: 0.6159\n",
      "Epoch 135/150\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.9465 - val_loss: 0.6158\n",
      "Epoch 136/150\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.9464 - val_loss: 0.6158\n",
      "Epoch 137/150\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.9463 - val_loss: 0.6157\n",
      "Epoch 138/150\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.9461 - val_loss: 0.6156\n",
      "Epoch 139/150\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.9460 - val_loss: 0.6156\n",
      "Epoch 140/150\n",
      "3/3 [==============================] - 0s 163ms/step - loss: 0.9459 - val_loss: 0.6155\n",
      "Epoch 141/150\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.9458 - val_loss: 0.6155\n",
      "Epoch 142/150\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 0.9456 - val_loss: 0.6154\n",
      "Epoch 143/150\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.9455 - val_loss: 0.6154\n",
      "Epoch 144/150\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.9454 - val_loss: 0.6153\n",
      "Epoch 145/150\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 0.9452 - val_loss: 0.6153\n",
      "Epoch 146/150\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.9451 - val_loss: 0.6152\n",
      "Epoch 147/150\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.9450 - val_loss: 0.6151\n",
      "Epoch 148/150\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.9449 - val_loss: 0.6150\n",
      "Epoch 149/150\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.9448 - val_loss: 0.6149\n",
      "Epoch 150/150\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.9447 - val_loss: 0.6149\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CDCD992D08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "prediction_data_long_10var=pm2d5_pred_model(train_data_long_10var, test_data_long_10var, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
